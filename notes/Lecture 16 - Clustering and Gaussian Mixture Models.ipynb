{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Clustering\n",
    "\n",
    "- Given $D = {x_n}^D$ and $K$, we want to output $k$ centroids and assignments for every data point to its cluster. Formally, we want to output ${\\mu_k}_{k=1}^{K}$ centroids and a function $A(x_n) \\in [1...K]$, the cluser assigned to point $x_n$. \n",
    "- We can define a distortion/objective function $J = \\sum_{n=1}^{N}\\sum_{k=1}^{K} r_{n,k}||x_n - \\mu_k||^2_2$ where $r_{n,k} \\in [0,1], r_{n,k} = 1$ iff $A(x_n) = k$. \n",
    "- This cost function basically sums over all n data points. For each point, we compute the distance to the $kth$ centroid that the data point is assigned to, and add that distance to our objective function. \n",
    "- The cost function is essentially summing up the L2 squared distances of (feature vector, assigned centroid) pairs. \n",
    "- The objective function is non-convex and there are exponentiall many possibilities to try out. It is in NP-hard. Since there are $K$ choices for each data point's cluster assignment and there are $n$ points, there are $n^K$ possible solutions that we must explore to get an exact solution. Therefore, we resort to heuristics to find a good enough solution in faster time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "### Minimizing k-means objective with Lloyd's Algorithm\n",
    "- The objective is once again: $J = \\sum_{n=1}^{N}\\sum_{k=1}^{K} r_{n,k}||x_n - \\mu_k||^2_2$ where $r_{n,k} \\in [0,1], r_{n,k} = 1$ iff $A(x_n) = k$. \n",
    "- Lloyd's algorithm, also called the k-means algorithm, is given as follows: \n",
    "    - Initialize ${\\mu_k}$ to some values. \n",
    "    - Assume the current values of ${\\mu_k}$ as fixed, minimize $J$ over ${r_{n,k}}$. TO do this, we just set $r_{n,k} =1$ where k is the index that mnimizes the distance from the nth training data point to the corresponding cluster. This is basically assigning points to clusters based on the current values of the centroids. \n",
    "    - Next, assume that the current value of $r_{n,k}$ is fixed and then minimize $J$ over $\\mu_{k}$ which leads to:\n",
    "    - $\\mu_k = \\frac{\\sum_n r_{n,k} x_n}{\\sum_n r_{n,k}}$, which is essentially the mean of the data points assigned to the $k$th cluster for all $k$. \n",
    "    - This is because differentiating, $\\nabla_{\\mu_k} J = 0$ gives $\\sum_n r_{n,k}(x_n - \\mu_k)2 = 0$, $\\sum_n r_{n,k}x_n - \\sum_n r_{n,k} \\mu_k = 0$, giving us the desired expression. \n",
    "    - The second step is basically recomputing the means. \n",
    "    - WE do this iteratively until stopping if J stays the same. \n",
    "\n",
    "- Basically, pick K random centroids/prototypes/means, assign each point to a centroid by computing the smallest distance, and then recompute cenntroids and repeat this unitl cost function J doesn't change. \n",
    "- How good is this k-means solution? \n",
    "- It convergres to a local minimum, since the objective is non-convex we are not going to find the optimal solution like we can iin linear regression. Therefore, the utility of k-means also depends on the random initiealization of the centroids at the beginning of the algorithm. \n",
    "- In practice, many different iterations of k-means ar ran on the same data, and we pick the best (with different random initializations). \n",
    "- K-means ++ is another interesting algorithm that has theoretical gaurantees on the success of k-means. \n",
    "- Increasing k decreases the optimal value of k-means objective. \n",
    "- K-means can be thought of picture compressing as well. We take a bunch of data such as a large picture that is $X \\in n * d$, and then run k-means to output a new representation $\\mu_k$ and $A(x_n)$, which reduces the space needed to represent the data. \n",
    "- For very high values of K, we don't lose much data and our compression is not very lossy, but we haven't compressed it by much. For very small values of K, we loes a lot more data and our compression is very lossy, but we reduced the filesize a lot\n",
    "- Intuitively, larger values for K will cause the obujective function to be lower and lower: If we have $K$ = the number of data points, then the objective will be zero. \n",
    "- This is analogous to verfitting in supervised learning\n",
    "- Intuition: look for the smallest $K$ at which the decrease becomes less significant. For example, if the decrease in the objective is a lot between 1 and 2, but very little (but still some) between 2 and 3 and 3 and 4, then $K = 2$ would be a sensible choice. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### K-medoids algorithm\n",
    "- K means comes with its problems: it's senstive to outliers, and in some applications we want the prototype s to be one of the points in the data themselves. Prototypes in general are what we call objects or points that reprsent our clusters. In general, these are mean values of the cluster, or in k-medoids, the \"mean\" point of specific clusters. \n",
    "- The algorithm is very similar to the k-means algorithm: \n",
    "    - Initialize $\\mu_k$ by randomly selecting $K/N$ data points.\n",
    "    - Assume that the current ${\\mu_k}$ is fixed, assign points to clusters with the following rule: \n",
    "        - $r_{n,k} = 1$ if $k = argmin_j || x_n - \\mu_j ||^2$ else $0$. \n",
    "    - In k-medoids, the prototype for a cluster is the data point closest to all other points in the cluster. Compute the average distance from all the points to the others in their clusters, and find the one who's distance is least. Formally, this is: \n",
    "        - $k * = argmin_m \\sum_n r_{n,k} || x_n - x_m ||^2$. \n",
    "- Stop if J doesn't change much\n",
    "\n",
    "- The main difference between K-mediods and K-means is that the centroids in K-mediods are data points themselves, while in K-means they are just points that accurately reflect the data in the cluster, not necessarily a point in the data itself. \n",
    "\n",
    "### Probabalistic Interpretation of K-means\n",
    "- Similar to linear regression's probabalistic interpretation that allowed us to write down linear regression as maximizing the likelihood, we can write a probabalistic interpretation of K-means that will allow us to understand it better and use it more powerfully. \n",
    "- Since the data forms clusters, we usuallly cannot model $p(x)$ with a single and known distribution. Instead we need to use multiple distributions. This leads to the idea of Gaussian Mixture Models. \n",
    "- Each region/cluster can be modelled with a distribution. Use gaussians, and we need to learn the clustering assignments (since we are working with unlabeled data), as well as the parameters (mean and covariance) of each of the distributions we are \"mixing\". \n",
    "- The basic idea is that we should write down a probability model that generates the data we have observed. We can model each region of the data with a distinct distribution and use Gaussian Mixture Models to combine these. \n",
    "\n",
    "### Defining GMMs\n",
    "- Let $p(x) = \\sum_{k=1}^{K} \\omega_k N(x | \\mu_k, \\sigma_k)$. This basically says that the kth gaussian is distributed normally with certain parameters, and has a weight $\\omega_k$ that contributes to the overall distribution. \n",
    "- $ \\forall k, \\omega_k > 0, \\sum_k \\omega_k = 1$ to ensure that $p(x)$ is a properly normalized pdf. \n",
    "- We can think of the GMM as the marginal distribution of a joint distrubtion. \n",
    "- Consider $p(x, z) = p(z) p(x | z)$. Here $x$ is our data and $z$ is which of the $k$ mixture components our data belongs to. So we have $z$ as a discrete random variable $\\in 1...K$. \n",
    "- We define the mixture weights: $\\omega_k = p(z = k)$. We also assume that the kth cluster's data is distributed normally, so that the conditional distributions are Gaussian: $p(x | z =k) = N(x | \\mu_k, \\sigma_k)$. \n",
    "- Since we can get the marginal distribution of x by summing over the joint probabilities, we have $p(x) = \\sum_k p(x,z)$ or $p(x) = \\sum_k p(z = k) p(x | z = k) $. Substituting in our mixture weights and our Gaussian distribution model, we obtain the marginal distribution as $ p(x) = \\sum_{k = 1}^{K} \\omega_k N(x | \\mu_k, \\sigma_k)$. \n",
    "\n",
    "### Example\n",
    "- Consider data clustered into red, blue, and green clusters. Then, the conditional distributions between x and z are $p(x | z = r) = N(x | \\mu_1, \\sigma_1)$, and similar distributions with different parameters for blue and green. \n",
    "- We can get the marginal distribution just by summing across the joint distributions: $p(x) = p(red)N(x| \\mu_1, \\sigma_1) + p(green) N(x | \\mu_2, \\sigma_2) + p(blue) N(x | \\mu_3, \\sigma_3)$.\n",
    "\n",
    "### Parameter likelihood estimation for GMMs\n",
    "- Now that we know how to write down a probabalistic model for these data, we can find the parameters that maximize the likelihood of observing the data that we have. We use the typical MLE format to find the best parameters. \n",
    "- First, we consider the simple and unrealistic case first: We have the labels $z$. Basically we assume a label z is observed for every x. \n",
    "- Let $D' = {x_n, z_n}_{n=1}^{N}$. This is the complete data - we have the labels. $D$ is what we actually have, the incomplete data. We can learn our parameters with MLE. \n",
    "- $\\theta = argmax log D' = \\sum_n log p(x_n, z_n)$. \n",
    "- The complete likelihood is decomposable: $\\sum_n log p(x_n, z_n) = \\sum_n log p(z_n) p(x_n | z_n)$. \n",
    "- Introduce a binary variable $\\gamma_{n,k} \\in (0,1)$ to indicate whether $z_n = k$. Then, we have $\\sum_n log p(x_n, z_n) = \\sum_k \\sum_n \\gamma_{n,k} log p(z = k) p(x_n | z = k)$. \n",
    "- Substituting our model (model weights and Gaussian Distribution), we have the likelihood as $\\sum_k \\sum_n \\gamma_{n,k}(log \\omega_k + log N(x_n | \\mu_k, \\sigma_k)$. \n",
    "- It can be shown that the maximum likelihood estimate is $\\omega_k = \\frac{\\sum_n \\gamma_{n,k}}{\\sum_k \\sum_n \\gamma_{n,k}}$, $\\mu_k = \\frac{1}{\\sum_n \\gamma_{n,k}}\\sum_n \\gamma_{n,k} x_n$, $\\sigma_k = \\frac{1}{\\sum_n \\gamma_{n,k}} \\sum_n \\gamma_{n,k}||x_n - \\mu_k||^2_2$. \n",
    "- Intuition: $w_k$ can be found by the number of data points who's $z_n = k$ and divide by the total number of data points. Remember $w_k = p(z_n = k)$.  \n",
    "- For $\\mu_k$ we get all the data points where $z_n = k$ and compute their mean. \n",
    "- For $\\sigma_k$ we get all the data points whose $z_n = k$ and compute their covariance matrix. \n",
    "- This intuition will be used to develop an algorithm for estimating the parameters even when we don't have the labels $z$. \n",
    "\n",
    "### Parameter restimation for GMMs: Incomplete data\n",
    "- When $z_n$ is not given, we don't know the probability of $p(z_n = k | x_n)$ directly, but we can apply Bayes rule to compute it. Bayes rule says that $p(x | y) = \\frac{p(y | x)p(x)}{p(y)}$\n",
    "- We get $ p(z_n = k | x_n) = \\frac{p(x_n | z_n = k)p(z_n =k)}{p(x_n)} = \\frac{p(x_n | z_n = k)p(z_n =k)}{\\sum_{k=1}^{K} p(x_n | z_n = k)p(z_n = k)}$.\n",
    "- We need to know the parameters $\\theta = ({\\mu_k}, {\\sigma_k})$ to compute the posterior probability. \n",
    "\n",
    "- Estiamtion with soft $\\gamma_{n,k}$: \n",
    "- Recall that $\\gamma_{n,k}$ was binary before. \n",
    "- Now its a soft assignment of $x_n$ to the $k$th component. \n",
    "- So now we've defined $\\gamma_{n,k} = p(z_n = k | x_n)$. Before, $\\gamma$ was binary, we knew what cluster it belonged to. Now its a probabilistic assignment. \n",
    "- With this, we get the same MLEs for $\\omega_k, \\mu_k, \\sigma_k$ but the associated $\\gamma_k$ is no longer binary. \n",
    "- This new estimation assumes that we know the parameters in the first place. But without this assumption, we can use an interative algorithm with the typical thought process: when you don't know two things and have to figure them out, randomly guess one and start learning from there. \n",
    "- Iterative procedure: \n",
    "    - Step 0: initialize $\\theta$ with some values (randomly)\n",
    "        - compute $\\gamma_{n,k}$ using current $\\theta$\n",
    "        - update $\\theta$ using the computed $r_{n,k}$\n",
    "        - repeat until the cost function does not change much\n",
    "        \n",
    "- Main difference from k-means: in k-means, we used hard assingment: assigning x to clusters. For GMMs, $r_{n,k}$ is probabalistic, so we give soft assignments to clusters.\n",
    "- GMMs: quantify probability that a given data point belongs to a cluster, given our current best guess for the parameters $\\theta$. \n",
    "- Will this algorithm converge? The answer comes from the Expectation-Maximization (EM algorithm) discussed later.\n",
    "\n",
    "### Summary\n",
    "- Clustering with k-means algorithm. \n",
    "- K-mediods is a variant that uses actual data points in the dataset as centroids. \n",
    "- GMMs are a probabalistic interpretation of K-means, while K-means does hard assignment to clusters, GMMs does a soft assignment based on the values of $\\gamma_{n,k}$ as determined by the maximum likelihood estimate. \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
