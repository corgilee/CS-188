{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "#### Review of previous lecture: kernelized ridge regression\n",
    "\n",
    "- Setup:\n",
    "- Input: $x \\in R^{D}$, feature transformation function $\\phi(x) \\in R^{M}$, output: $y \\in R$, Hypothesis/model $h_w(x) = w^T\\phi(x)$\n",
    "- $\\phi(x)$ is a nonlinear basis function\n",
    "- 2 ways to do ridge regression:\n",
    "1) Minimize $J$ by learning $\\hat{w} = (\\Phi^T\\Phi + \\lambda I)^{-1} \\Phi^{T}y$\n",
    "- prediction: $y = w^{T}\\phi(x)$. \n",
    "- remember that $\\Phi$ is the design matrix of dimension $N x M$ and each row of it is given by $\\phi(x_n)$\n",
    "- Learning cost: $O(D^{4}N + D^{6})$ (can you see why?)\n",
    "- Prediction cost: $O(D^2)$. \n",
    "- So learning and predicting both vary greatly and directly with $D$, the dimensionality of our original feature set. \n",
    "\n",
    "#### Kernelized Dual ridge regression: another way\n",
    "- Learn $\\alpha$ by minimizing cost function $J(\\alpha)$: \n",
    "- $\\hat{\\alpha} = (K + \\lambda I)^{-1}y$\n",
    "- Predict for a new input x: $y^{T}(K + \\lambda I)^{-1} k_x$\n",
    "- $K$ is the kernel matrix given by $ \\Phi\\Phi^{T} $ and $ k_x = \\Phi \\phi(x)$\n",
    "- We have the design matrix $\\Phi = \\begin{bmatrix} \\phi(x_1)^{T} \\\\ \\phi(x_2)^{T} \\\\ ... \\\\ \\phi(x_n)^{T} \\\\ \\end{bmatrix}$\n",
    "- So $K = \\Phi \\Phi^{T}$ is an $N x N$ matrix and $k_x$ is given by a column vector $\\Phi \\phi(x) = \\begin{bmatrix} \\phi(x_1)^{T}\\phi(x) \\\\ \\phi(x_2)^{T}\\phi(x) \\\\ ... \\\\ \\phi(x_n)^{T}\\phi(x) \\end{bmatrix} $\n",
    "\n",
    "- **Importantly**, $K$ and $k_x$ can be computed without knowledge of the design matrix $\\Phi$. We can compute each $K_m,n$ as an inner product of our original features: $(1 + x_{m}^{T}x_{n})$ and similarly, $k_{x,n} = (1 + x_{n}^{T}x)^{2}$.\n",
    "- Cost of learning: $O(N^{3} + N^{2}D) $, cost of prediction: $ O(N^{2})$. The kernelized algorithm scales with $N$, the number of training examples that we have, while the previous algorithm scales with the dimensionality of our features. The kernel trick lets us be more efficienet and also represent an infinite dimension feature transformation. \n",
    "- If we have a bunch of features (even infinite), use the kernel methods. \n",
    "\n",
    "\n",
    "#### Support Vector Machines\n",
    "- Most commonly used classification algorithms\n",
    "- Good generalization & can be kernelized. \n",
    "\n",
    "Intuition: The decision boundary: \n",
    "- Assuming for now a perfectly linearly seperable dataset, we want to pick a decision boundary such that the distance is maximized from both the classes. IE, the decision boundary is as far away as possible from both classes while still maintaining 100% accuracy. \n",
    "\n",
    "- Hyperplane given by $w^{T}x + b = 0 $. \n",
    "- Computing the signed distance from a point $a$ not on the hyperplane to the hyperplane by taking the projection of the vector $a - a_0$, where $a_0$ in on the hyperplane, onto the hyperplane: $dist(w, a) = \\frac{w}{||w||}(a - a_0) = \\frac{w^{T}a - w^{T}a_0}{||w||} = \\frac{1}{||w||}(w^{T}a + b)$.\n",
    "- The unsigned distance from a point $\\phi(x)$ to the hyperplane is $ \\frac{|w^{T}\\phi(x) + b|}{||w||}$\n",
    "- But since the hyperplane perfectly separates the data, any particular $x_n, y_n$ will have the same sign, so this can also be written as $ \\frac{y_n(w^{T}\\phi(x) + b)}{||w||} $\n",
    "\n",
    "- Denote the margin as the smallest distance between the hyperplane and all training points. Then, $m(w, b) = min_n \\frac{y_n(w^{T}x_n + b)}{||w||} $ and since we want a decision boundary that is as far away as possible from all training points, we pick parameters that maximize the margin: \n",
    "- $max_{w,b} \\frac{1}{||w||} min_{n} y_n(w^{T}\\phi(x_n) + b) $\n",
    "- Note: scaling the parameters $w, b $ both by a constant factor $c$ does not change the decision boundary since $(cw)^{T}\\phi(x) + cb = c(w^{T}\\phi(x) + b) = c(0) = 0 $, so we can further constrain the problem such that $min_n y_n(w^{T}\\phi(x_n) + b) = 1$\n",
    "- This scaling is done because the previous function was not convex, but now we can frame SVM classification as a constrained convex optimization problem. \n",
    "\n",
    "- We must now solve $max_{w,b} \\frac{1}{||w||^2} s.t. min_n y_n(w^{T}\\phi(x_n) + b) = 1 = max_{w,b} \\frac{1}{||w||^2} y_n(w^{T}\\phi(x_n) + b) \\geq 1, n \\in [1..N]$. \n",
    "- This is the same as solving $min_{w,b} \\frac{1}{2}||w||^{2} s.t. y_n(w^{T}\\phi(x_n) + b) \\geq 1, n \\in [1...N]$. \n",
    "- This is called a max-margin classifier. \n",
    "\n",
    "#### Constrained convex optimization problem (convex program)\n",
    "- $min_{n} f_0(x) s.t. f_i(x) \\leq 0, i = 1,...m$ (inequality constraint) or $min_{n} f_0(x) s.t. a_{i}^{T}x = b_i, i = 1,...m$ (equality constraint)\n",
    "- where $f_0, ... f_m$ are all convex functions. if a point satisifies all constraints, it is called feasible. If no point satisfies all constraints, it is called infeasible and there is no solution to the constrained convex optimization problem. \n",
    "- SVM constrained optimization problem, assuming perfect linear seperability: \n",
    "$min_{w,b} \\frac{1}{2} || w||^{2} s.t. -y_n[w^{T}\\phi(x_n) + b] + 1 \\leq 0, n \\in [1...N]$.\n",
    "- The objective function is convex in the parameters, and the inequality constraints are linear and therefore convex in the parameters. Hence, this is a convex opt problem which can be solved to get us the optimal sets of parameters. \n",
    "\n",
    "#### Constrained Optimization in non-seperable setting\n",
    "- Usually the data are non-separable\n",
    "- Modify to account for this: introduce slack variables: $\\zeta \\geq 0 $\n",
    "- Now $ y_n(w^{T}\\phi(x) + b) \\geq 1 - \\zeta, n \\in [1...N]$\n",
    "- if you misclassify, the lhs is negative so we increase the size of our slack variables so that our constraint is still satisfied. But obviously we don't want to let our slack variables get too large, as this will lead to a bad model. \n",
    "- Control slack variable size by incorporating them into the optimization problem:\n",
    "- $min_{w,b,\\zeta} \\frac{1}{2} ||w||^{2} + C\\sum_{n} \\zeta_{n}$, s.t. $y_n(w^{T}\\phi(x_n)+b) \\geq 1 - \\zeta_{n}$, $\\zeta_{n} \\geq 0, n \\in [1...N]$\n",
    "- Large C: pick small slack variables, smaller C = less control on slack variable size. \n",
    "- Similar to regularization constant in ridge regression: $C = \\frac{1}{\\lambda}$\n",
    "\n",
    "#### Meaning of Support vectors in SVM\n",
    "- Consider the slack variables $\\zeta$. Support vectors can have slack variables that are $\\geq 1, \\leq 1$, and $0$.\n",
    "- The linear decision boundary can be computed with only the support vectors, which form a small portion of the entire dataset. The rest of the data does not determine the optimal linear decision boundary. \n",
    "- The support vectors are data points that are misclassified (always have a slack variable $\\zeta > 1$), data points that are correctly classified but directly on the margin ($ \\zeta = 0$), or classified correctly but within the margin ($ 1>\\zeta > 0$)\n",
    "\n",
    "#### Alternate derivation of SVM: Hinge Loss\n",
    "- Generally, we minimize functions like the sum of squared errors/empirical risk with a regularization term. However, there's also the Hinge Loss. \n",
    "- Assume $y \\in {-1,1}$ and the decision rule is given by $h(x) = sign(a(x)) $ where $a(x) = w^{T}\\phi(x) + b$. Then $l(y, a(x)) = max(0,1-ya(x))$.\n",
    "- This not only looks at whether we were right or wrong, but also looks at our confidence of our estimators. \n",
    "- We can then minimize the hinge loss: \n",
    "- $min_{\\theta} \\sum_{n} max(0,1 - y_{n}(w^{T}\\phi(x_n) + b)) + \\frac{\\lambda}{2} ||w||^{2}$\n",
    "- It turns out that this can be written as equivalent to the previous geometric interpretation. \n",
    "- The SVM can be given by a constrained convex optimization problem: \n",
    "- $min_{w,b, \\zeta} \\frac{1}{2} ||w||_2^{2} + C \\sum_{n}\\zeta_n $ s.t. $y_n(w^{T}\\phi(x_n) + b) \\geq 1 - \\zeta_n, n \\in [1...N]$ and $\\zeta_n \\geq 0, n \\in [1...N] $\n",
    "- If we have the optimal values for $w$ and $b$, how do we get the optimal $\\zeta_n$ slack variables? \n",
    "- If we classify correctly, then $y_n(w^{T}\\phi(x_n) + b) \\geq 1$ and $\\zeta_n = 0 $. Otherwise, the minimum value for $\\zeta_n$ is $\\zeta_n = 1 - y_n(w^{T}\\phi(x_n) + b) $. \n",
    "- Therefore, $\\zeta_n = max(0, 1 - y_n(w^{T}\\phi(x_n) + b))$ and the model becomes the equivalent of minimizing the (l2-regularized) hinge loss:\n",
    "- $min_{w,b} \\frac{1}{2} ||w|^{2} + C \\sum_{n} max(0, 1 - y_n(w^{T}\\phi(x_n) + b)) $\n",
    "- let $C = \\frac{1}{\\lambda}$ and multiply the whole thing by $\\lambda$:\n",
    "- $min_{w,b} = \\frac{\\lambda}{2} ||w||^{2} + \\sum_{n} max(0, 1 - y_n(w^{T} \\phi(x_n) + b)) $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
