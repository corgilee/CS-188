{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens when $X^TX$ is not invertible? \n",
    "\n",
    "1. N < D, not enough data to estimate all parameters or 2. X columns are not linearly independent, features are correlated, so solution is not uniqye. \n",
    "\n",
    "Solution: $\\Theta = (X^TX + \\lambda I)X^Ty$. $I \\in R^{D+1} $ is the identity matrix scaled by a constant $\\lambda > 0$. $\\lambda I$ is always invertible, so even if $X^TX$ is not invertible, if we add on $\\lambda I$, it will become invertible. \n",
    "\n",
    "- Invertibility: all eigenvalues are non-zero. $X^TX$ by itself is a positive semidefinite matrix, meaning that all its eigenvalues are greater than or equal to 0. If an eigenvalue is 0, however, then we can't invert the matrix. \n",
    "\n",
    "- To show that $X^TX$ is PSD, we must show that for all vectors $z$, $z^TX^TXz \\geq 0$, but since this is $(Xz)*(z^TX)$, this is always greater than or equal to 0. \n",
    "\n",
    "- On the other hand, the quantity $ (X^TX + \\lambda I) $ is positive definite, meaning that all eigenvalues are strictly greater than 0 and the matrix is invertible. \n",
    "\n",
    "- As a result of this, we have a new objective function: $J(\\theta) = || y - X\\Theta|| + \\lambda \\sum_{d=1}^{D} \\theta_d^{2} $. The second term becomes small when the values of the parameters are small. \n",
    "\n",
    "- The second term is essentially adding the L2 norm of the weights times a constant, called the regularization hyperparameter. \n",
    "\n",
    "- The benefits of doing this L2-regularizaiton/ridge regression is that the solution is numerically stable since the matrix is invertible, and we prevent overfitting. \n",
    "\n",
    "- Tradeoffs: L2-term prefers small weights, while the error term wants to pick weights that yield the smallest error. The value of $\\lambda$ determines this tradeoff. $\\lambda$ is a hyperparameter, which we can use CV to tune. If we set $\\lambda$ to be very high, then we will learn weights that are very small. If we set $\\lambda$ to be very low or zero, then we don't regularize at all. \n",
    "\n",
    "- When we use nonlinear regression, we run the risk of overfitting with a large polynomial. When we overfit, the parameters become very large. \n",
    "- Preventing overfitting:\n",
    "- Use more training data, even if we have a higher degree polynomial, it's less \"wiggly\". \n",
    "- Regularization methods. What if we don't have more training data? \n",
    "\n",
    "Regularization methods: \n",
    "- Goal: identify \"simpler\" models. Simpler = less parameters, more parameters are zero, parameters are smaller in magnitude. \n",
    "- Simpler functions are smoother: nearby values of $x$ have a similar output $y$.\n",
    "\n",
    "- Consider two vectors $x$ and $x'$ that differ only in their first component by a small value $e$. \n",
    "- then, their predictions $y$ and $y'$ differ by $ew_1$. A smaller $w_1$ means similar predictions for vectors that are not too far apart. \n",
    "- Intuitively, this makes sense - we want similar outputs for similar values. So we prefer lower values of parameters, and this is where regularization comes in. \n",
    "\n",
    "### Regularized linear (ridge) regression\n",
    "\n",
    "- $ J(w,b) = \\sum_{n} (y_n - w^{T}x_n - b)^2 = \\lambda ||w||^2 $, where $\\lambda \\geq 0 $. he extra term is called the regularizer and controls the model complexity. If $\\lambda$ becomes large, we ignore the error term so w approaches 0, and if it is small, then we ignore teh regularization term. \n",
    "- Gradient: $\\frac{\\delta J}{\\delta \\Theta} - 2(X^TX\\theta - X^Ty + \\lambda \\theta) $, solution: $\\Theta = (X^TX + \\lambda I)^{-1} + X^{T}y $. \n",
    "- discourages parameters that are too large. \n",
    "\n",
    "- We can't tune $\\lambda$ on training dataset, since this will set it to 0 which defeats the purpose. For different values of $\\lambda$ when we train, we will get different parameters which we can use on a validation set to test which value is the best. CV Stuff. \n",
    "\n",
    "### Overview So Far\n",
    "\n",
    "- We have a risk function $R[h(x)] = \\sum_{(x,y)} L(h(x), y)p(x,y) $ which we seek to approximate with the empirical risk $ h(x) = \\frac{1}{N} \\sum_{n} L(h(x), y) $. As N approaches infinity, the empirical risk intuitively approaches the true risk. \n",
    "\n",
    "- Minimizing the empirical risk, or empirical risk minimization, can be problematic: as our hypothesis function $h(x)$ becomes more and more complicated, the training error goes to 0 but then we overfit and have poor generalization. \n",
    "- Instead, we use the regularizer to prevent overfitting, just adding a regularization term to the cost/objective function. \n",
    "\n",
    "- Several types of regularizers: \n",
    "L2: $\\lambda ||W||^2 $, L1: $\\lambda ||W||$, 0-norm: $\\sum_{d=1}^{D} 1 $, p-norm: $ (\\sum_{d=1}^{D} w_{d}^{p})^{\\frac{1}{p}} $\n",
    "- L1 prefers sparse representations where many weight values are 0. \n",
    "\n",
    "### Framework for ML/supervised learning: \n",
    "- Model/hypothesis\n",
    "- Pick loss function based on problem/assumptions. \n",
    "- Regularizer\n",
    "- Algorithm to solve optimization problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
