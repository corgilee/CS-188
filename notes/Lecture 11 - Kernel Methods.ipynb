{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Kernel Methods\n",
    "\n",
    "- Motivation: linear models are convenient. They're computationally efficient and simple, but we'd like them to be more expressive (but not too expressive since it may overfit). \n",
    "- Use non-linear basis function $\\phi(x) : R^{D} \\rightarrow{} R^{M}$\n",
    "- Key idea: instead of choosing a function $\\phi(x)$ and computing it, we can choose an equivalent **kernel function** that relies only on inner products of the training examples that is more efficient to work with. \n",
    "- Example: \n",
    "- Let $\\phi(x) = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ ... \\\\ x_D \\\\ x_1^2 \\\\ x_1x_2 \\\\ ... \\\\ x_D^{2} \\end{bmatrix} $\n",
    "- Then the size of $\\phi(x)$ is $M = O(D^2) $, so computing the model $w^{T}\\phi(x)$ takes $O(D^2)$ time. \n",
    "- However, many learnign algorithms can be rewritten to depend only on an inner product of two training instances through $\\phi(x_i)^{T}\\phi(x_j) = (1 + x_i^{T}x_j)^2$, which takes only $O(D)$ time. \n",
    "\n",
    "### Example: Kernelizing ridge regression\n",
    "- Ridge regression model: $J(w) = \\sum_{n} (y_n - w^{T}\\phi(x_n))^2 + \\lambda ||w||^{2}$\n",
    "- The solution is given by $\\nabla J = 2 \\sum_{n}(y_n - w^{T}\\phi(x_n))(-\\phi(x_n)) + 2\\lambda w $\n",
    "- The optimal weight vector is a linear combination of the features: $\\hat{w} = \\sum_{n} \\frac{1}{\\lambda}(y_n - w^{T}\\phi(x_n))\\phi(x_n) = \\sum_{n}\\alpha_n\\phi(x_n) = \\Phi^{T}\\alpha $\n",
    "- **Key Equation:** $w = \\Phi^{T} \\alpha$, where $\\Phi$ is the design matrix of *transformed* faetures and $\\alpha$ is a linear combination of the features. \n",
    "- The kernel matrix $K = \\Phi \\Phi^{T} = \\begin{bmatrix} \\phi(x_1)^{T}\\phi(x_2) && ... && \\phi(x_n)^T\\phi(x_1) \\\\ ... && ... && ... \\\\ \\phi(x_n)^{T}\\phi(x_2) && ... && \\phi(x_n)^{T}\\phi(x_n)  \\end{bmatrix}$\n",
    "- $K$ is symmetrix PSD and $K_{m,n}$ can be computed with an inner product $\\phi(x_m)^{T}\\phi(x_n)$\n",
    "- Then, the optimal $\\alpha$ can be computed with $\\hat{\\alpha} = (K + \\lambda I)^{-1}y $\n",
    "- Importantly, we only need to know the kernel matrix to compute $\\alpha$, the exact form of $\\phi(x)$ is not important as long as we know how to get the inner products. This will give rise to the kernel functions. \n",
    "- Computing the parameter vector does require knowledge of $\\Phi$: $\\hat{w} = \\Phi^{T}\\alpha$\n",
    "- Predictions however, don't need knowledge of the particular form of $\\phi(x)$, they only require inner products: $w^{T}\\phi(x) = y^{T}(K + \\lambda I)^{-1}\\Phi \\phi(x) = y^{T}(K + \\lambda I)^{-1} k_x$, where $K = \\Phi{\\Phi}^{T}$ is the design matrix and $k_x$ is the column vector given by $\\Phi \\phi(x)$.\n",
    "\n",
    "### Kernel functions\n",
    "- The polynomial nonlinear basis function's inner products $\\phi(x_m)^{T}\\phi(x_n)$ can be written as inner products of the original features: $x_m^{T}x_n$, so we can compute these inner products in terms of only the original, non transformed features. \n",
    "- Intuition for kernel functions: \n",
    "- To do kernelized ridge regression, we only need to compute an inner product between a test point and any training point. We need this for any possible pair of points, so we need a function that takes a pair of points and computes an inner product. \n",
    "- This is the kernel function $k$. Given two inputs, $k$ tells us how similar or close these inputs are in terms of the feature space $\\phi$. \n",
    "- Examples of kernel functions: polynomial with degree d: $k(x_m, x_n) = (x_m^{T}x_n + c)^{d} $\n",
    "- Gaussian/rbf kernel: $ k(x_m, x_n) = e^\\frac{-||x_m - x_n||_2^2}{2\\sigma^2}$\n",
    "- Gaussian kernel only depends on L2 squared distance between two inputs, corresponds to a feature space with infinite dimensions (but we can work directly with original features). \n",
    "- Kernels have hypermaraters that need to be tuned: $d,c, \\sigma^2$.\n",
    "- A kernel function is a bivariate function that satisfies $k(x_m, x_n) = k(x_n, x_m) = \\phi(x_m)^{T}\\phi(x_n) $ for some function $\\phi$.\n",
    "- Also, a function $k$ is a kernel function if the kernel matrix $K$, who's elements are given by $ K_{m,n} = k(x_m, x_n)$ is PSD, or has all eigenvalues greater than or equal to 0. \n",
    "- So the kernel matrix $K$'s components $K_{m,m}$ can be given by either $\\phi(x_m)^{T}\\phi(x_n)$ or $k(x_m, x_n)$\n",
    "\n",
    "### Picking kernels to use\n",
    "- if $k(x_m, x_n)$ is a kernel then if $c > 0$, $ck(x_m, x_n)$ is also a kernel. \n",
    "- if both $k_1(x_m, x_n)$ and $k_2(x_m, x_n)$ are kernels and $\\alpha, \\beta \\geq 0 $, then $\\alpha k_1 + \\beta k_2$ is also a kernel. \n",
    "- the product of two kernels is a kernel, a kernel raised to the power e is also a kernel. \n",
    "- choosing a kernel is an art, people typically start off with Gaussian/RBF and incorporate domain knowledge. \n",
    "- Hyperparameters still need to be tuned. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Recap \n",
    "- Sometimes computing the nonlinear basis function $\\phi(x)$ can be expensive, so when learning algorithms rely on their inner products $\\phi(x_m)^{T}\\phi(x_n)$, we can use the kernel trick to compute this as an inner product of only the original feature vecors, such as $(x_m^{T}x_n + c)^{d} $ which is faster. \n",
    "- The design matrix $K = \\Phi \\Phi^{T}$ can be given by the nonlinear basis function or from the kernel functions. So we can predict and learn without knowing the specific nonlinear basis function. \n",
    "- The $k$th column vector of $K$ is given by $\\Phi \\phi(x)$. \n",
    "- the optimal $\\alpha$ can be computed with $\\hat{\\alpha} = (K + \\lambda I)^{-1}y $\n",
    "-  Predictions only require the inner products: $ w^{T}\\phi(x) = y^{T}(K + \\lambda I)^{-1} k_x$, where $K = \\Phi{\\Phi}^{T}$ is the design matrix and $k_x$ is the column vector given by $\\Phi \\phi(x)$.\n",
    "- $k$ is a kernel function if the entire kernel matrix is PSD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Evaluating ML Algorithms\n",
    "- Different types of accuracy\n",
    "- True positive, true negative, false positive, false negative. \n",
    "- Accuracy = $\\frac{TP + TN}{P + N}$\n",
    "- Error = 1 - acc\n",
    "- True Positive Rate = Recall = Sensitivity = $\\frac{TP}{P} $\n",
    "- This is the probability of classifying a spam email as spam, for example. \n",
    "- Specificity = $\\frac{TN}{N}$ - this is the probability of classifying a non-spam email as non-spam. \n",
    "- False positive rate = 1 - specificty\n",
    "- Precision = Positive prediction value = $\\frac{TP}{TP + FP}$\n",
    "- Probability that an email classified as spam really is spam. The denominator is all the emails classified as spam, and the top is all emails that were actually spam. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Different Types of Kernels\n",
    "- linear: $K(x_i, x_j) = x_i^{T}x_j$\n",
    "- polynomial kernel: $K(x_i, x_j) = (c + \\gamma x_i^{T}x_j)^{d}$, where $d$ and $c$ are real numbers. \n",
    "- exponential kernel: $K(x_i, x_j) = e^{-\\gamma || x_i - x_j ||^2}, \\gamma > 0$\n",
    "- sigmoid kernel: $K(x_i, x_j) = tanh(\\gamma x_i^{T}x_j + r)$\n",
    "- a linear combination of kernels is also a kernel\n",
    "- constant $c > 0$ times a kernel is a kernel\n",
    "- product of two kernels is a kernel\n",
    "- exponentiation of kernel is a kernel\n",
    "- For any $f(x) \\in R$, $f(x) K$ is also a kernel. \n",
    "- Examples of kernel functions: polynomial with degree d: $k(x_m, x_n) = (x_m^{T}x_n + c)^{d} $\n",
    "- Gaussian/rbf kernel: $ k(x_m, x_n) = e^\\frac{-||x_m - x_n||_2^2}{2\\sigma^2}$\n",
    "- Gaussian kernel only depends on L2 squared distance between two inputs, corresponds to a feature space with infinite dimensions (but we can work directly with original features). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
