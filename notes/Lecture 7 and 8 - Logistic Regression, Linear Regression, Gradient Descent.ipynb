{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Lecture 7: Logistic Regression **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- Instead of trying to predict direct (binary) classes like in the case of the vanilla perceptron, we can try to predict the probability of a certain instance $x$ belonging to a particular class. \n",
    "\n",
    "- This is where the idea of the logistic regression classifier comes in:\n",
    "    - Still binary classification\n",
    "    - Inputs: $x \\in R^{D} $, output: $ y \\in {0, 1} $\n",
    "    - Instead of directly predicting the label, we first want to obtain probabilities for the input x corresponding to a particular class, and then using these probabilities to make the prediction.\n",
    "    - **Model**: $ h_{w,b}(x) = p(y = 1 | x; b,w) = \\sigma(b + w^{T}x) $, where $\\sigma$ is the sigmoid function: $ \\frac{1}{1 + e^{-a}} $ that maps real valued inputs to values in between 0 and 1. In this case, $ a = w^{T}x + b $\n",
    "    - Properties of the sigmoid function: \n",
    "        - Bounded between 0 & 1 -> interpretable as probabilities\n",
    "        - Monotonically increasing: usable to derive classification rules: if > .5, predict 1, if < .5, predict 0. \n",
    "        - Nice computational properties, such as being to easily express the derivative of the sigmoid.\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- **Linear or nonlinear classifier?**\n",
    "    - Even though $ \\sigma(a) $ is nonlinear, the decision boundary is determined by whether $ \\sigma(a) < 0.5 $ or not, which is a function that is linear in x. \n",
    "    - Our **learning problem** is that we're given some training data $D = {(x_1, y_1), ... (x_n, y_n)}$ and we want to learn a function $ h_{w,b}: x \\longrightarrow y $ by finding the best parameters w and b. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- **Review: Likelihood function:**\n",
    "    - $h(x)$ defines the probability that y = 1 for some instance x, and this probability is dependent on the parameters w and b.\n",
    "    - Let $x_1, ... x_n$ be independent and identically distributed with PDF $ p(x_i ; \\theta) $. Then the likelihood function is $ L(\\theta) = p(x_1, ... x_n) = \\Pi_{i = 1}^{N}p(x_i;\\theta)$\n",
    "    - The maximum likelihood estimator $\\hat{\\theta}$ is the parameter that maximizes the likelihood, or the joint probabiltiy of the n random variables. \n",
    "    - The log-likelihood (as well as any constant times the likelihood) has the same maximum. Also, the benefits of having logs include simplifying mathematical expressions and helping with numerical stability/underflow issues. So, we like logs and are going to take the log of our likelihood, giving us the log-likelihood to work with. \n",
    "    - The likelihood indicates how likely we are to observe the data $x_1, ... x_n$ given some value for $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- **Deriving the log-likelihood for logistic regression**\n",
    "    - From our model, we obtain probabilities after passing our linear combination to the sigmoid function. These probabilities correspond to probabilties for y being 1, so we can define:\n",
    "    - $ p(y_n | x_n; w,b) = h_{w,b}(x)^{y_n} $ if $ y = 1$, and $(1-h_{w, b}(x))^{1 - y_n} $ else.\n",
    "    - Compactly, this is:\n",
    "    - $ p(y_n | x_n; w,b) = h_{w, b}(x)^y_{n}(1 - h_{w, b}(x))^{1 - y_n} $. This is the probability of a single instance, label pair. For the likelihood, we are interested in the joint probability, in other words:\n",
    "    - $ L( (x_1, y_1), ... (x_n, y_n); w,b) $ where $ y $ ~ $Ber(h_{w,b}(x))$:\n",
    "        $ L( (x_1, y_1), ... (x_n, y_n); w,b) = \\Pi_{i = 1}^{N} h_{w,b}(x)^{y_n}(1 - h_{w,b}(x))^{1 - y_{n}} $\n",
    "    - The log-likelihood is:\n",
    "        $l( (x_1, y_1), ... (x_n, y_n); w,b) = \\sum_{i = 1}^{N} y_n log (h_{w,b}(x)) + (1-y_n) log(1 - h_{w,b}(x)) $\n",
    "    - It is often convenient to refer to the negative log-likelihood:\n",
    "    $J( (x_1, y_1), ... (x_n, y_n); w,b) = -\\sum_{i = 1}^{N} y_n log (h_{w,b}(x)) + (1-y_n) log(1 - h_{w,b}(x)) $\n",
    "    - Often, the bias trick is used to absorb the bias term into the weights, and write the negative log-likelihood in terms of a new vector of parameters \\theta instead of w and b together. This is simply for convenience. \n",
    "    - maximizing a function is the same as minimizing the negative of that function. So when we study optimization problems, we can limit ourselves to minimization for consistency. A common algorithm is gradient descent.\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Lecture 8: Gradient Descent & Linear Regression **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- Input: $ x \\in R^{D} $\n",
    "- Output: $ y \\in [0,1] $\n",
    "- Training data: $ D = {(x_n, y_n)}$, n = 1,2,...N\n",
    "- Model: $ h_{w,b}(x) = p(y = 1 | x;w,b) = \\sigma(a(x)) $, where $ a(x) = w^{T}x + b $\n",
    "\n",
    "- How do we find the optimal parameters for logistic regression? \n",
    "- Minimize the negative log-likelihood: \n",
    "    $J(\\theta) = -\\sum_{n} y_n log(h_{\\theta}(x_n)) + (1 - y_n)log(1 - h_{\\theta}(x_n))$\n",
    "- In general, these functions that we minimze to get the best parameters are known as the cost functions. In the case of logistic regression, this function is the negative log-likelihood. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Gradient descent algorithm:\n",
    "\n",
    "    Start at a random point. \n",
    "    Repeat:\n",
    "            Determine a descent direction\n",
    "            Choose a step size\n",
    "            Update parameters\n",
    "    Until Stopping condition is satisfied.\n",
    "\n",
    "Does gradient descent converge to the absolute minimum? To study this, we need to consider two types of functions: convex and non-convex functions. \n",
    "\n",
    "Convex functions: \n",
    "- All chords between points (a, f(a)) and (b, f(b)) lie above the value of the function. \n",
    "- A function is convex if the following holds: $f(\\lambda a + (1 - \\lambda)b) \\leq \\lambda f(a) + (1 - \\lambda)f(b), \\forall 0  \\leq \\lambda \\leq 1 $\n",
    "- This means that in between an arbitrary interval (a, b) which are in the domain of f, we pick a point that lies in between a and b. The RHS denotes a line that connects a & b via the line going through (a, f(a) and (b, f(b)), while the LHS denotes the value between f(a) and f(b) on the function. \n",
    "- Basically if we plot the function and no chords between 2 points go underneath the value of the function, it is convex. \n",
    "\n",
    "Determining convexity: \n",
    "- Single variables: $ f''(x) \\geq 0 $ everywhere\n",
    "- Examples: linear functions, $x^2$, $e^x$, $1/x, x > 0$\n",
    "\n",
    "Determining convexity for multivariable functions: \n",
    "- Get a matrix of second order derivatives, known as the Hessian. \n",
    "$ H = \\begin{bmatrix}\n",
    "       \\frac{d^2f(x)}{dx_{1}^{2}} & \\frac{d^2f(x)}{dx_{1}dx_{2}} \\\\\n",
    "       \\frac{d^2f(x)}{dx_{2}dx_{1}} & \\frac{d^2f(x)}{dx_{2}^{2}}\n",
    "\\end{bmatrix} $\n",
    "- Important to note about the Hessian: it's a matrix that is defined at particular points, which means in general $H$ has particular values that change depending on what values of the function we are considering. This is similar to the general second derivative for single-variable functions: it changes depending on where we are in the function. \n",
    "\n",
    "- If the Hessian is positive semidefinite, then f is convex. \n",
    "- A matrix H is positive semidefinite iff $z^{T}Hz = \\sum_{j,k} H_{j,k}z_{j}z_{k} \\geq 0$ where z is any arbitrary vector. \n",
    "\n",
    "- Example:\n",
    "    - Let $f(x_1, x_2) = x_{1}^{2} + 2x_{2}^{2}$. Then $H = \\begin{bmatrix} 2 & 0 \\\\ 0 & 4 \\end{bmatrix}$ And the product $z^{T}Hz = 2z_{1}^{2} + 4z_{2}^{2} \\geq 0 \\forall z$ so the Hessian is positive semidefinite and the function is convex. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Use the following iterative process for gradient descent:\n",
    "- initialize $\\theta_{1}^{0} and \\theta_{2}^{0}$ randomly, and let $t = 0$\n",
    "- do:\n",
    "    $$ \\theta_{i}^{t + 1} := \\theta_{i}^{t} - \\alpha \\frac{\\delta J}{\\delta \\theta_i}$$\n",
    "- until $f(\\theta_{1}, \\theta_{2})$ doesn't change much: $| f(\\theta_{new}) - f(\\theta_{old}) | <\\epsilon$\n",
    "\n",
    "In the implementation of gradient descent, don't actually update all the weights at once. Compute all gradients with respect to the previous values, and then do a batch update of all weights at once. This is more efficient and is what is done in practice. \n",
    "\n",
    "Remarks:\n",
    "- alpha is often called the step size: how far our update will go along in the negative direction of the gardient. \n",
    "- With a suitable choice for alpha, the procedure converges to a stationary point where $\\frac{\\delta f}{\\delta\\Theta} = 0 $\n",
    "- A stationary point is only necessary for a minimum to exist, not a sufficient condition. But if f is convex, then at the stationary point we know that we are both at a local and global minimum. \n",
    "\n",
    "Gradient of the negative log-likelihood: \n",
    "- $J(\\theta) = -\\sum_{n} y_n log(h_{\\theta}(x_n)) + (1 - y_n)log(1 - h_{theta}(x_n))$\n",
    "- It is a good exercise to show that: \n",
    "$ \\frac{\\delta}{\\delta \\Theta_{j}} = \\sum_{i = 1}^{m} [(h_{\\Theta}(x_i) - y_i)x_{i,j}] $\n",
    "- This is for j = 1, 2, ...m, giving us a vector of m partial derivatives, the number of parameters there are. \n",
    "- Notably, we can think of $ h_{\\theta}(x_i) - y_i $ as the error for a particular training example. Then, for a particular partial derivative j, its value is the sum over all training errors, each of which is multiplied by the jth feature of the ith vector\n",
    "\n",
    "Algorithm:\n",
    "- Gradient Descent($J(\\theta$)):\n",
    "    - t = 0\n",
    "    - initialize $\\theta^{0}$\n",
    "    - do:\n",
    "        - $$\\theta_{j}^{t + 1} := \\theta_{j}^{t} - \\alpha\\sum_{i = 1}^{m} [(h_{\\Theta}(x_i) - y_i)x_{i,j}]$$\n",
    "        - until convergence. \n",
    "        \n",
    "- Step size choice is essential for convergence. If the step size is too large, then the algorithm may overshoot the actual global minimum and diverge/oscillate around. If the step size is too small, the algorithm may converge very very slowly. Generally, an algorithm known as line search is used to determine the step size, and the step size/learning rate is gradually decreased (decayed) during the running of the algorithm, especially towards the end of learning. \n",
    "\n",
    "- For logistic regression (which models a conditional distribution), there is no closed form solution so we must rely on numerical optimization techniques such as gradient descent.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Linear Regression **\n",
    "\n",
    "- Key difference: different way to quantify errors if we want to predict a real valued label instead of binary/categorical label. This implies different learning algorithms. \n",
    "- model example:\n",
    "    - house sale price = a(square_footage) + b(num_rooms) + ... + noise that can't be explained by our model. \n",
    "    \n",
    "- Regression predicts continuous outputs. Two ways of quantifying error: absolute difference or squared difference. The goal is to adjust the model so that the sum of the squared errors (which indicates what is unexplainable from model, also known as the residual) is as asmall as possible. \n",
    "\n",
    "Setup: \n",
    "- Inputs: $x \\in R^{D}$, Outputs: $y \\in R$\n",
    "- Hypothesis/model: $h_{w,b}(x) = b + w^{T}x $\n",
    "\n",
    "- How do we learn the parameters? By minimizing the sum of squared errors. \n",
    "- Cost function: residual sum of squares:\n",
    "$ ||y_{i} - h_{\\theta}(x)|| = \\sum_{i = 1}^{N} (y_i - h_{\\theta}(x_i))^2$\n",
    "\n",
    "Gradient descent for linear regression is very similar to gradient descent for logistic regression\n",
    "t:=0\n",
    "- Initialize $\\theta^{0}$\n",
    "- repeat:\n",
    "    - $\\theta_{j}^{t+1} := \\theta^{t} - \\alpha \\frac{\\delta J}{\\delta \\theta_j}$\n",
    "    - $ t:= t+1$\n",
    "- until convergence\n",
    "\n",
    "Importantly, your modelling assumptions come into play when working with gardient descent and computing the cost function. For example, the assumption that we desire to be as close as possible to the predict value, and therefore, it is reasonable to minimize the cost function. In some other applications, however, we may just want to ensure we never make any underestimates. Then, we'd like to optimize the cost function such that its parameters underestimate the real valued output only with very low expected probability. \n",
    "\n",
    "\n",
    "The residual sum of squares also has an analytical solution. We have\n",
    "- $J(\\theta) = \\sum_{i = 1}^{N} (y_i - (\\theta_0 + \\theta_1x_i))^2 $\n",
    "- to identify stationary points, we can differentiate with respect to the two parameters and set them equal to. \n",
    "- $ \\frac{\\delta J(\\theta)}{\\delta \\theta_{0}} = -2 \\sum_{i=1}^{n} ((y_{i}- (\\theta_0 + \\theta_1x_i)) $\n",
    "- $ \\frac{\\delta J(\\theta}{\\delta \\theta_{1}} = -2 \\sum_{i = 1}^{n} x_i(y_i - (\\theta_0 + \\theta_1x_i)$\n",
    "- Setting them equal to 0, we obtain the analytical solution for the parameters that minimize the residual sum of squares:\n",
    "- $\\theta_0 = -\\theta_1\\bar{x} + \\bar{y}$\n",
    "- $\\theta_1 = \\frac{\\sum(x_n - \\bar{x})(y_n - \\bar{y}}{\\sum(x_i - \\bar{x})^2}$\n",
    "\n",
    "Analytical solution for multiple variables: \n",
    "\n",
    "Our model is $ y = X\\Theta$\n",
    "- If $X$ is invertible, this means that all its rows and columns are independent. We can easily find w by explicitly solving for it by taking the inverse of x. \n",
    "- Otherwise, if $X$ is not invertible, this implies that its rows and columns are not independent. We now want to obtain $y_{pred}$ that is the closest approximation to the true $y$. In other words, we want to minimize the residual squared error: $ ||y - y_{pred}||^{2} = || y - X\\theta ||^{2} = \\sum_{i = 1}^{N}(y_i - X_i\\theta) $. \n",
    "- There is an explicit way to solve for this with linear algebra, the proof of which is interesting (hint: use projections): \n",
    "- $ y = X\\theta$\n",
    "- $X^{T}y = X^{T}X\\theta$\n",
    "- $ \\theta = (X^{T}X)^{-1} X^{T}y $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
