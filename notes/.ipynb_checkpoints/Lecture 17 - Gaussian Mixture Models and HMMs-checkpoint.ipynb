{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Review of Clustering\n",
    "- Intuition: data is assigned to cluster k should be close to the mean for that cluster $\\mu_k$. \n",
    "- Distortion measure: $J = \\sum_n \\sum_j r_{n,k} || x_n - \\mu_k||_2^2$. \n",
    "- We have the indicator variables $r_{n,k} = 1$ iff $A(x_n) = k$. \n",
    "- Minimize distortion measure thorugh altenative optimization between $r_{nk}$ and $\\mu_k$. \n",
    "- Lloyd Algorithm: \n",
    "    - Step 0: initialize $\\mu_k$. \n",
    "    - Step 1: Update $r_{nk}$  based on $\\mu_k$ (basically assign data to clusters)\n",
    "    - Step 2: Recompute means $\\mu_k$. \n",
    "\n",
    "### GMMs: Mixture Models - Probabalistic Interpretation\n",
    "- GMM = probabilistic interpretation of k-means\n",
    "- Model each distribution witha  gaussian. \n",
    "- GMM: $p(x) = \\sum_{k=1}^{K} w_{k} N(x | \\mu_k, \\sigma_k)$ or equivalently, $p(x) = \\sum_k p(z_n = k)N(x | \\mu_k, \\sigma_k)$. \n",
    "- We have $\\mu_k$ and $\\sigma_k$ which are the means and covariances of the kth component. \n",
    "- $w_k$ are the mixture weights. We have $w_k \\geq 0 \\forall k$, and $\\sum_k w_k = 1$. \n",
    "- Given incomplete data $D = [x_n]$ we must learn parameters of the Gaussian distributions for each of our mixture components, as well as the values $w_k$ that give us weights for our mixture components. \n",
    "\n",
    "### Parameter estimation for GMMs \n",
    "- The parameters are the mixture weights as well as means and covariances of each particular Gaussian distribution: $\\theta = {w_k, \\mu_k, \\sigma_k}, k \\in 1...K$. \n",
    "- If we knew these parameters, then we can compute $P(x_n | z_n = k)$ by just substituing in the relevant normal distribution: $P(x_n | z_n = k) = N(x_n | \\mu_k, \\sigma_k)$. \n",
    "- With Bayes theorem, we can compute the posterior probability $P(z_n = k | x_n)$ (also remember that the prior $w_k = P(z_n = k)$). The quanity $P(z_n = k | x_n)$ basically tells us the probability with which the cluster $x_n$ is assigned to cluster $k$. \n",
    "- Bayes theorem states that $P(a | b) = \\frac{P(b |a) P(a)}{P(b)}$. \n",
    "- Applying this, we have $P(z_n = k | x_n) = \\frac{P(x_n | z_n = k)P(z_n = k)}{P(x_n)}$. \n",
    "- Rewriting this with the chain rule, we have $P(z_n = k | x_n) = \\frac{P(x_n | z_n = k)P(z_n = k)}{\\sum_k p(x_n | z_n = k) p(z_n = k)}$. We can compute this quantity if we assume we know all of the parameters. \n",
    "\n",
    "- If we assume we have the labels and not the parameters, then we have the problem of finding the parameters given the complete data, ie, the points $x_n$ and the associated cluster assignments $z_n$. \n",
    "- We can do this by maximizing the complete likelihood: $\\theta = argmax log P(D') = \\sum_n log p(x_n, z_n)$. \n",
    "- We obtain some pretty intuitive values for the parameters (explicity written in the last lecture). $w_k$ is just the fraction of all of the data points who's $z_n = k$, $\\mu_k$ is the mean of all points who's $z_n = k$, and $\\sigma_k$ is the covariance matrix of all points who's $z_n = k$. \n",
    "\n",
    "### Parameter estimation for GMMs: Incomplete Data\n",
    "- The interesting problem is when we only have observed training data $D = {x_n}$ and the cluster assignments $z_n$ are hidden. \n",
    "- We have a similar goal of obtaining the maximum likelihood estimate of the parameters: $\\theta$\n",
    "- This is $argmax_{\\theta} \\sum_{n} log p(x_n | \\theta)$. Tis new objective function $l(\\theta)$ is called the incomplete log-likelihood. \n",
    "\n",
    "### Optimization with the EM Algorithm\n",
    "- No easy/typical way to optimize the incomplete log likelihood. \n",
    "- Expectation maximization algorithm: a strategy for iteratively optiziming this function. \n",
    "- Two steps as its applied to GMMS: \n",
    "    - E-step: guess vcalues for $z_n$ given values of parameters $\\theta$. \n",
    "    - M-Step; obtain new values for the parameters $\\theta$ given the newly computed values for $\\z_n$. \n",
    "\n",
    "- E-step: Soft cluster assignments\n",
    "    - Define $\\gamma_{nk} = p(z_n = k | x_n, \\theta)$. This is the posterior of $z_n$ given $x_n$ and $\\theta$. \n",
    "    - In the complete data setting, $\\gamma$ was binary but now its a soft/probabilistic assignment of $x_n$ to the $k$th component, so $x_n$ is being assigned to each component with some probability. \n",
    "    - Given $\\theta = {w_k, \\mu_k, \\sigma_k}$ we can compute $\\gamma_{nk}$ with Bayes theorem: \n",
    "    - $\\gamma_{nk} = p(z_n = k | x_n) = \\frac{p(x_n | z_n = k) p(z_n = k)}{p(x_n)}$ where we can expand the bottom using the chain rule. \n",
    "\n",
    "- M-step: \n",
    "    - Maximize the complete likelihood. \n",
    "    - Previously, we had $\\gamma_{nk}$ as binary but now we define $\\gamma_{nk} = p(z_n = k | x_n)$, soft cluster assignments from the previous step\n",
    "    - We now compute the maximum likelihood estimation in the same fashion as when we would have complete data, $argmax_{\\theta} \\sum_n p(x_n, z_n)$. We end up with the same expressions for the MLEs as before (written in the previous lecture notes), but now the gammas are probabalistic instead of just binary. \n",
    "    - Intuition: each point contributes some fractional componentnt oe ach of the parameters, with weights determined by $\\gamma_{nk}$. \n",
    "    \n",
    "- EM procedure for GMM: \n",
    "    - Alternate between estimating $\\gamma_{nk}$ and $\\theta$.\n",
    "    - Initialize $\\theta$ with some values (random or otherwise). \n",
    "        - Repeat: E-step: compute soft assignment gammas, M-step: maximize complete likelihood to obtain MLEs for $\\theta = (w_k, \\mu_k, \\sigma_k)$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### GMMs and K-means\n",
    "- GMM = probabalistic interpretation of K-means. \n",
    "- GMMs reduce to K-means under some specific conditions. In this case, the EM algorithm for GMMs also simplies down to Lloyd's algorithm. These conditions are: \n",
    "    - Assume that all Gaussians have $\\sigma^2I$, in other words, their covariance matrices are diagonal. \n",
    "    - Further assume that the covariance tends to 0, so we only need to estimate $\\mu_k$, so we have less parameters. - Therefore, GMMs can be considered a more general model. K-means = hard GMM or GMM = soft k-means. \n",
    "    \n",
    "### EM Algorithm\n",
    "- With the EM algorithm, the estimates of the parameter in the m-step of each iteration increases the likelihood. \n",
    "- The algorithm converges but only to a local optima. \n",
    "\n",
    "### Hidden Markov Models\n",
    "- Motivation: Many models have assumptions. Linearly separable data: can be removed w/nonlin bsis functions and kernels. We also typically assume that the data are independently distributed - can we challenge this assumption? It is not realistic for many cases, such as any time-series data. \n",
    "\n",
    "- Sequential process: \n",
    "- A system that can occupy one of K states. \n",
    "- Let $X_t$ be the state of the system at time $t$. \n",
    "- $X_t$ is a random variable that takes values between 1 and K. \n",
    "- If we observe the state of the system from time 1 to tt, what is its state at time t+1?\n",
    "- It's given by the conditional distribution: \n",
    "- $P(x_{t+1} | x_1 ... x_t)$. \n",
    "- What is the probability of observing that the system is in states $x_1 ... x_t$ at times 1 to t? $P(x_1 ... x_t) = P(x_1) P(x_2 | x_1) P(x_3 | x_2, x_1) ... P(x_t | x_1 ... x_{t-1})$. Basically the probability of a current state at a particular time is dependent on all previous states. \n",
    "- If we knwon all the conditional probabilities we can compute the joint. \n",
    "- This is kind of difficult (but possible with things like RNNs and LSTMs/GRUs), but for Markov Processes, we just assume that the next state depends only on the current state. The future is conditioned on the present and independent of the past: $P(x_{t+1} | x_1 ... x_t) = P(x_{t+1} | x_t)$. \n",
    "\n",
    "### Markov Chains\n",
    "- Initial probabilities to begin the chain: $\\pi_i = P(X_1 = i)$. \n",
    "- Transition probabilities: $q_{ij} = P(X_{t+1} = i | X_t = j)$. \n",
    "- For the transition probability matrix and the intial probability vector we require that the cols sum to 1. \n",
    "- Computing on Markov Chains: \n",
    "- What is the probability of observing a particular sequence of states such as $X_1, X_2, X_3 = 1,1,3$? \n",
    "- More generally: $P(x_1 .. x_T) = \\pi_{x_1} q_{x_1, x_2} ... q_{x_{T-1}, x_T}$. This gives us the probability of observing a particular sequence of states. \n",
    "- We may also be interested in computing the probability of a particular state at a particular time. \n",
    "- This means that we want to compute $P(x_T = i)$, and we want to consider all previous states that could have led to this current state. \n",
    "- We know that a particular sequence $P(x_1 ... x_T) = \\pi_{x_1} q_{x_2, x_1} ... q_{x_{t+1}, x_t}$ so we just have to consider all possible such sequencees that end up with the desired end state. \n",
    "- This value is $P(X_t = i) = \\sum_{x_1, x_2, ... x_{t-1}} P(X_1, x_2, ... x_{t-1}, x_t = i)$. Since each $x_t$ can take $K$ states, the computational cost of this is $O(K^T)$. \n",
    "- However, there is an algorithm to compute this more efficiently. \n",
    "- We want to compute $P(x_T = i)$. To do this, we first define $p_t(i) = P(X_t = i)$. For the first state, we have $p_{t=1}(i) = 1$ if $i$ is the sart state else 0.\n",
    "- NOw assume that we know $p_{t-1}(i) \\forall i$. Then to compute the next time in the time series, $p_t(i)$, we just have $p_t(i) = \\sum_j p_{t-1}(j)q_{ij}$. \n",
    "- The computational cost of this is $O(K^2)$ to compute each $p_t(i)$ for a given $t$, and doing this $T-1$ times gives a total computation cost of $O(K^2(T-1))$, a much better computational cost than exponential. This is an example of dynamic programming. \n",
    "\n",
    "### Hidden Markov Models\n",
    "- Previously, we directly observed the states $X_t$. Now we actually bserve another random variable $Y_t$ that is affected by $X_t$. \n",
    "- In this case we referred to $Y_t$ as the observed states and $X_t$ as the hidden states. Having observed $Y = (Y_1 ... Y_t)$ we want to ask questions about the hidden states. \n",
    "- We define the set of observed states/emission symbols $B = b_1 ... b_L$ to be the values that the observed states can take. \n",
    "- We define emission probabilities: the probability of observing a state given a particular hidden state. This is $e_k(b) = P(Y_t = b | X_t = k)$. We require $\\sum_{b} e_k(b) = 1$. \n",
    "- Hidden Markov Model definitions: \n",
    "    - Hidden and observed states. Some initial probability $\\pi_i =  P(X_1 = i)$. Transition probabilities $q_{ij} = P(X_t = i | X_{t-1} = j)$. Emission probabilities $e_k(b) = P(Y_t = b | X_t = k)$. \n",
    "\n",
    "### Querying HMMs\n",
    "- HMMs can be generative models: \n",
    "    - Pick a state $X_1$ according to the distribution $\\pi$. \n",
    "    - let t = 1\n",
    "    - Emit an observation $y_T$ according to $e_x(t)$. \n",
    "    - Choose the next state $x_{t+1}$ according to the distribution $q_{x_t}$. \n",
    "    - $t+=1$ and repeat. \n",
    "    \n",
    "- We may also be interested in the joint porbability of a sequence of hidden and observed states. We have $P(y, x) = P(y_{1:T}, x_{1:T}) = P(y_{1:T} | x_{1:T}) P(x_{1:T}) = \\prod_{t=1}^{T} e_{x_t}(y_t) \\pi_{x_1}\\prod_{t=1}^{T-1}q_{x_{t+1}, x_t}$. \n",
    "- HMMs most probable path (MPP) problem\n",
    "- Given a sequence of obesrvations $y_1 ... y_T$ what is the most probable sequence of hidden states $x_1 ... x_t$ ? \n",
    "- We basically want $argmax_{x_{1:T}} P(y_{1:T}, x_{1:T})$, or the sequence of hidden states that maximize the probability of observing what we have observed. Since from above we already know how to compute a specific sequence $P(y_{1:T} | x_{1:T}) P(x_{1:T})$ we can solve this problem by searching over all possible values of $x_{1:T}$. Since each $X_t$ can take on $K$ values, there are $O(K^T)$ possibilities so this search would take exponential time, which is bad. \n",
    "- However, there is an efficient DP solution to this. \n",
    "\n",
    "### The Viterbi Algorithm\n",
    "- Remember that the MPP problem is trying to find the most probable sequence of hidden states. This is $argmax_{x_{1:T}} P(y_{1:T}, x_{1:T})$. \n",
    "- Suppose that we have computed the probability $v_t(k) \\forall t  \\in 1...T$ and $k \\in 1...k$. This is the probability of the MPP for observations $y_1 ... y_t$ so the path has length $t$ that ends up in state $k$. \n",
    "- Suppose that we have computed the probability $v_t(k) \\forall t \\in 1...T, k \\in 1...K$. \n",
    "- The probability of the MPP for observations $y_1 ... y_T$ (so that the path has length t and ends up in state k): \n",
    "- $v_t(k) = max_{x_{1:t-1}} P(y_t, x_{1:t-1}, x_t = k)$. \n",
    "- If we look at $v_T(k)$, it tells us the probability of the MPP of length $T$ that ends in state $K$. \n",
    "- The answer to the MPP problem is then just $max_k v_T(k)$. \n",
    "- Can we compute $v_t(k)$ efficiently? \n",
    "- Assume that we have computed $v_{t-1}(l)$. \n",
    "- This means that we have computed the probability for the MPP of length $t-1$ that ends in state $l$ for all values of $l$. \n",
    "- How do we use this to compute the probability of length t that ends in state k? \n",
    "\n",
    "- Viterbi Algorithm: \n",
    "    - Start with t = 1. \n",
    "    - $v_1(k) = max_{x_1 = k} P(y_1, x_1 = k) = P(y_1 | x_1 = k )P(x_1 = k) = e_k(y_1)\\pi_k$. \n",
    "- Now assume that we've computed $v_{t-1}(l)$. THis menans that we have computed the probability of the MPP of length $t-1$ that end up in state $l$, for all values of $l$. \n",
    "- How do we use this to compute the probability of MPP of length $t$ that ends up in state $k$ ? \n",
    "- The most probable path wih last 2 states $l,k$ is the MPP with state $l$ at time $t-1$ followed by a transition from state $l$ to state $k$, and emitting the observation at time $t$. \n",
    "- The probaiblity of this path is $v_{t_1}(l) P(x_t = k | X_{t-1} = l)P(y_t | X_t = k)$.\n",
    "- = $v_{t-1}(l)q_{lk}e_t(y_t)$. \n",
    "- The MPP that ends in state $k$ at ttime $t$ is obtained by maximizing over all possible states $l$ in teh previous time $t-1$. $v_t(k) = max_l v_{t-1}(l) q_{kl}e_t(y_t)$.  \n",
    "\n",
    "- So basically the Viterbi algorithm boils down to 2 things: \n",
    "- Base case: $ v_1(k) = max_{x_1 = k} P(y_1, x_1 = k)$ which can be easily expanded. \n",
    "- The MPP that ends in state $k$ at time $t$ is given by maximizing over all possible states $l$ in the previous time $t-1$: \n",
    "    - $v_t(k) = max_l v_{t_1} (l) = q_{kl}e_t(y_t)$. \n",
    "\n",
    "### Learning HMMs\n",
    "- previously assumed that the parameters are knwon. Can we actually learn these from data? \n",
    "- Params of HMMs: $\\theta = (\\pi, Q, E)$. \n",
    "- Here $E$ is the matrix of memission probabilities $E_{kb} = e_k(b)$. \n",
    "- Given training data of observed states, find parameters $\\theta$ that maximize the log-likelihood. \n",
    "- We have incomplete data: observed states $D = y_{1:T}$ and unobserved states $x_{1:T}$. \n",
    "- We have $\\hat{\\theta} = argmax_{\\theta} l(\\theta) = argmax_{\\theta} log P(y_{1:T} | \\theta)$. \n",
    "- This is $argmax_\\theta \\sum_{x_{1:T}} log P(y_{1:T}, x_{1:T} | \\theta)$\n",
    "- The objective is called the incomplet elog-likelihood. \n",
    "- Can be optimized with EM algorithm, like we did for GMMs. \n",
    "\n",
    "### HMM Summary\n",
    "- Allows us to model dependencies. \n",
    "- Can efficiently perform computations using DP\n",
    "- Learn params with EM algorithm. \n",
    "- Pretty common model. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
