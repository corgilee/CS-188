{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Constrained Optimization Review\n",
    "- Convex Constrained optimization problem: \n",
    "- $min_x f_0(x)$ s.t. $f_i(x) \\leq 0, i = [1...M]$, $f_j(x) = 0, j = [1...N]$ \n",
    "- Define the lagrangian by introducing variable corresponding to each of the M+N constraints. \n",
    "- $L(x, \\alpha, \\beta) = f_0(x) + \\sum_n \\alpha_i f_i(x) + \\sum_m \\beta_j f_j(x)$\n",
    "- If you consider $max_{\\alpha, \\beta, \\alpha_i \\geq 0} L$, the value is $\\infty$ if $x$ violates a primal constraint, otherwise the value of the objective is exactly $f_0(x)$. \n",
    "- So computing $min_x max_{\\alpha, \\beta, \\alpha_i \\geq 0} L$ gives us the same answer as the original primal problem, denoted as $p*$. \n",
    "- Switching the order of maximization and minimization leads us to the dual problem. \n",
    "- Primal for soft margin SVM: $min_{w, b, \\zeta} C \\sum_n \\zeta_n + \\frac{1}{2}||w||_2^2$ s.t. $ 1 - \\zeta_n - y_n[w^T\\phi(x_n) + b] \\leq 0, n = [1...N]$, and $-\\zeta_n \\leq 0, n = [1...N]$. \n",
    "- We can immediately write down the lagrangian (see previous notes for this). \n",
    "- Then, we have the solution to the primal problem $p* = min_{w, b, \\zeta} max_{\\alpha, \\lambda, \\alpha_i \\geq 0, \\alpha_i \\geq 0} L(w, b, \\zeta, \\alpha, \\lambda) $\n",
    "- The solution to the dual problem is given by maximizing over the primal variables first, then minimizing over the dual: $d* = max_{\\alpha, \\lambda} min_{w, b, \\zeta} L(w, b, \\zeta, \\alpha, \\lambda)$. \n",
    "- In general, we have weak duality: $d* \\leq p*$ but for the SVM, since we have $f_0$ and $f_i$ are convex functions and $h_j$ (the equality constraints) are affine (aka linear, but with an extra intercept term), then there is strong duality: $d* = p*$. \n",
    "- The solution to the dual is given by $max_\\alpha \\sum_n \\alpha_n - \\frac{1}{2}\\sum_{m,n} \\alpha_m \\alpha_n y_m y_n \\phi(x_m) \\phi(x_n) $ s.t. $\\sum_n \\alpha_n y_n = 0$ and $0 \\leq \\alpha_n \\leq C$ for $n \\in 1...N$\n",
    "- We can find the primal weights if we have knowledge of the function $\\phi$: $ w= \\sum_n \\alpha_n y_n \\phi(x_n)$\n",
    "- Also, the KKT conditions hold: $\\lambda_n \\zeta_n = 0$ and more importantly $\\alpha[ 1 - \\zeta_n - y_n(w^T\\phi(x_n) + b)] = 0$.\n",
    "- If $\\alpha_n > 0$, then it contributes to $w$ which characterizes the hyperplane learned. The feature vector corresponding to $\\alpha_n$ is then known as one of the support vectors. \n",
    "- Since $\\alpha_n > 0$ for support vectors, we require $ 1 - \\zeta_n - y_n(w^t\\phi(x_n) + b) = 0$ giving us $\\zeta_n$ conditions for support vectors. Simply put, support vectors are training data that are misclassified, classified correctly but within the margin, or classified correctly on the margin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
