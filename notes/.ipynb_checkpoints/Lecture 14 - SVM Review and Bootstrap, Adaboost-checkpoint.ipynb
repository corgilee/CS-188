{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Constrained Optimization Review\n",
    "- Convex Constrained optimization problem: \n",
    "- $min_x f_0(x)$ s.t. $f_i(x) \\leq 0, i = [1...M]$, $f_j(x) = 0, j = [1...N]$ \n",
    "- Define the lagrangian by introducing variable corresponding to each of the M+N constraints. \n",
    "- $L(x, \\alpha, \\beta) = f_0(x) + \\sum_n \\alpha_i f_i(x) + \\sum_m \\beta_j f_j(x)$\n",
    "- If you consider $max_{\\alpha, \\beta, \\alpha_i \\geq 0} L$, the value is $\\infty$ if $x$ violates a primal constraint, otherwise the value of the objective is exactly $f_0(x)$. \n",
    "- So computing $min_x max_{\\alpha, \\beta, \\alpha_i \\geq 0} L$ gives us the same answer as the original primal problem, denoted as $p*$. \n",
    "- Switching the order of maximization and minimization leads us to the dual problem. \n",
    "- Primal for soft margin SVM: $min_{w, b, \\zeta} C \\sum_n \\zeta_n + \\frac{1}{2}||w||_2^2$ s.t. $ 1 - \\zeta_n - y_n[w^T\\phi(x_n) + b] \\leq 0, n = [1...N]$, and $-\\zeta_n \\leq 0, n = [1...N]$. \n",
    "- We can immediately write down the lagrangian (see previous notes for this). \n",
    "- Then, we have the solution to the primal problem $p* = min_{w, b, \\zeta} max_{\\alpha, \\lambda, \\alpha_i \\geq 0, \\alpha_i \\geq 0} L(w, b, \\zeta, \\alpha, \\lambda) $\n",
    "- The solution to the dual problem is given by maximizing over the primal variables first, then minimizing over the dual: $d* = max_{\\alpha, \\lambda} min_{w, b, \\zeta} L(w, b, \\zeta, \\alpha, \\lambda)$. \n",
    "- In general, we have weak duality: $d* \\leq p*$ but for the SVM, since we have $f_0$ and $f_i$ are convex functions and $h_j$ (the equality constraints) are affine (aka linear, but with an extra intercept term), then there is strong duality: $d* = p*$. \n",
    "- The solution to the dual is given by $max_\\alpha \\sum_n \\alpha_n - \\frac{1}{2}\\sum_{m,n} \\alpha_m \\alpha_n y_m y_n \\phi(x_m) \\phi(x_n) $ s.t. $\\sum_n \\alpha_n y_n = 0$ and $0 \\leq \\alpha_n \\leq C$ for $n \\in 1...N$\n",
    "- We can find the primal weights if we have knowledge of the function $\\phi$: $ w= \\sum_n \\alpha_n y_n \\phi(x_n)$\n",
    "- Also, the KKT conditions hold: $\\lambda_n \\zeta_n = 0$ and more importantly $\\alpha[ 1 - \\zeta_n - y_n(w^T\\phi(x_n) + b)] = 0$.\n",
    "- More generally, the KKT conditions are that at the optimal values, $\\frac{\\delta L}{\\delta x} = 0, \\frac{\\delta L}{\\delta \\alpha} = 0, \\frac{\\delta L}{\\delta \\beta} = 0$, the values are optimal (obviously), and complementary slackness holds, which means that $\\alpha_i f_i(x) = 0, i \\in [1...N]$ and $\\beta_j f_j(x) = 0, j \\in [1...N]$. \n",
    "- If $\\alpha_n > 0$, then it contributes to $w$ which characterizes the hyperplane learned. The feature vector corresponding to $\\alpha_n$ is then known as one of the support vectors. \n",
    "- Since $\\alpha_n > 0$ for support vectors, we require $ 1 - \\zeta_n - y_n(w^t\\phi(x_n) + b) = 0$ giving us $\\zeta_n$ conditions for support vectors. Simply put, support vectors are training data that are misclassified, classified correctly but within the margin, or classified correctly on the margin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Ensemble Methods\n",
    "- Consider a set of predictors/base learners $h_1...h_L$. We can combine their predictions to hopefully get a more accurate predictor $H$. \n",
    "- This might work when the predictors make different types of mistakes. Basically, if we have predictors that are based on different sets of assumptions, they will produce different kinds of things that are learned and predicted, and thus also different mistakes. \n",
    "- The first way to do this can be having multiple classifiers on the same data. \n",
    "- Train $h_1...h_L$ on a given training dataset, and then combine their predictions: $ h = sign(\\sum_i h_i) $, $h_i \\in { + 1, -1} $. \n",
    "- Or, we could have a weighted majority vote, if we think that some classifiers are more important than other classifiers: $h = sign(w_1h_1 + ... w_L h_L) $. \n",
    "    - to learn these weights, we can compute weights on a validation set to find the best setting of the weight parameters. \n",
    "- If we are doing regression instead of binary classification, we can use the mean, median, or weighted mean to combine predictions.\n",
    "\n",
    "### Training same classifier on multiple datasets\n",
    "- Split orignal training dataset into multiple datasets, train classifer on each. \n",
    "- Each classifier is trained on a small dataset, so this does not generalize very well. \n",
    "\n",
    "\n",
    "\n",
    "### Bagging/Boostrap Aggreagting\n",
    "- Bootstrap resampling: training data with $N$ instances: $D$. \n",
    "- Create $B$ bootstrap training datasets: $D_1, ... D_B$. \n",
    "- Each $D_b$ contains $N$ training examples drawn randomly from $D$ with replacement. \n",
    "- Train a classifier $h_b$ on each of $D_b$, take the majority vote. \n",
    "- Basically, we construct a bunch of datasets that are the same size as the original training dataset, but we sample from the original with replacement, with leads to random duplicates in each of our new sampled training datasets. Then, we train individual classifiers on each of these (different from each other) datasets, and combine their predictions. \n",
    "\n",
    "### Adaboost Algorithm\n",
    "- High-level ideas: combine lots of classifiers \n",
    "- construct/identify these classifiers one at a time\n",
    "- use weak/base classifiers to arrive at complex decision boudnarise (strong classifiers)\n",
    "- Adaboost takes an ensemble of weak/base classifiers and turns them into a single strong classifier\n",
    "- Given: $N$ training examples ${x_n, y_n}$ where $y_n \\in {+1, -1}$ and some way of constructing weak classifiers (ie, decision stumps)\n",
    "- Initialize weights $w_1(n) = \\frac{1}{N}$ for every tarining sample. \n",
    "- For t = 1...T (a hyperparameter): \n",
    "    - Train a weak classifier $h_t(x)$ using the current weights $w_t(x)$ by minimizing the weighted classification error: $\\epsilon_t = \\sum_n w_t(n)I(y_n != h_t(x_n)) $\n",
    "    - Compute contribution for this classifier to overall strong classifier: $\\beta_t = \\frac{1}{2}log(\\frac{1 - \\epsilon_t}{\\epsilon_t})$. If we have a more accurate classifier, that means $\\epsilon_t$ is small and $\\beta_t$ is higher. Similarly, if we have a more random classifier, $\\epsilon_t$ approaches $0.5$ and $\\beta_t$ approaches 0. \n",
    "    - Update weights for training points: $w_t(n+1) = w_t(n)e^{-\\beta_t y_n h_t(x_n)} $. If we predicted incorrectly for a particular training point, this means that we increase its associated weight, and if we predicted correctly for a particular training point, this means that we decrease its associated weight. To make sure that the weights don't blow up, we normalize them such that $\\sum_n w_{t + 1}(n) = 1$. \n",
    "- Output final classifier: $h(x) = sign( \\sum_{t = 1}^{T} \\beta_t h_t(x)$. Each term in the sum is a weak classifeir for every iteration multiplied by a value that indicates how important it is/how much it contributes to the overall strong classifier learned. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
