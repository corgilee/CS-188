{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Neural Networks and Deep Learning\n",
    "- Neural nets are quite powerful\n",
    "- with a sifficient number of nonlinear hidden units, can approximate any function. \n",
    "- Activation functions: \n",
    "    - sigmoid\n",
    "    - ReLu/piecewise linear\n",
    "    - tanh\n",
    "- Can be used for regression, binary classification, or softmax/multiclass classification. \n",
    "\n",
    "- Regression: Linear output $y_k = \\sum_k w_{kj}^2 (\\sum_i w_{ij}x_i)$. \n",
    "- Binary classification: $y = \\sigma(w_2^Th(w_1^Tx))$\n",
    "- Softmax/multiclass classification: $z_k = softmax(w_2^Th(w_1^Tx_n + b_1) + b_2)$. \n",
    "- For classification, minimize the cross entropy loss: $min - \\sum_n \\sum_k y_{nk} log f_k(x_n)$. \n",
    "- Hard/nonconvex optimization problem\n",
    "- Stochastic gradient descent\n",
    "- Randomly pick a training example and update the gradient and update only based on that point. \n",
    "- Other modifications: \n",
    "    1. Can use minibatch learning\n",
    "    - initialization is important, learning rate is important. \n",
    "    - Momentum/nesterov momentum: remebering previous direction of gradients for new updates. \n",
    "- Have to tune many hyperparameters, such as number of hidden layesr and number of hidden units. \n",
    "\n",
    "### Computing Gradients with Backpropagation \n",
    "- Calculate feedforward output from input\n",
    "- Compute error E based on predictions and actual value. \n",
    "- Backprop the error through the network by weighhing it by the gradients of theactivation functions and weihts in th previous layer\n",
    "- From this, compute $\\frac{dE}{dw}$ and update the weights. \n",
    "- Suppose the net computes $z_k = g_k(b_k + \\sum_j g_j(b_j + \\sum_i x_iw_{ij})w_{jk})$ where $g$ is the nonlinear function and the inner sums are just linear combinations. \n",
    "- Further, let the error function be the simple sum of squared errors: $ E = \\frac{1}{2} \\sum_{k} x_k - t_k$. \n",
    "- We can write out the above function as 2 steps \n",
    "    - First, computing the activation of the hidden layer: \n",
    "        $z_j = g_j(\\sum_i x_iw_{ij} + b_j)$\n",
    "    - Next, computing the activation of the output layer:\n",
    "        $z_k = g_k(\\sum_j z_j w_{jk})$\n",
    "- The derivative $\\frac{dE}{dw_{jk}}$ will give us the rate of change of our error with respect to our outer layer of weights, which we can use for gradient descent. \n",
    "- Just apply the chain rule: \n",
    "- $\\frac{dE}{dw_{jk}} = (z_k - t_k)\\frac{d}{dw_{jk}} (z_k - t_k) = (z_k - t_k)g'k(\\sum_j z_jw_{jk})z_j$.\n",
    "- This has an intuitive meaning: the rate of change of our output layer is just our prediction error times the derivative of what the output layer function computed, multiplied by what was fed into our output layer. \n",
    "- We can do the same for the first layer of weights, just have to backprop more. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
