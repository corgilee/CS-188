\documentclass[11pt]{article}

\newcommand{\cnum}{CS188}
\newcommand{\ced}{Winter 2017}
\newcommand{\ctitle}[3]{\title{\vspace{-0.5in}\cnum, \ced\\Problem Set #1: #2\\Due #3}}
\usepackage{enumitem}
\usepackage{graphicx}
\newcommand{\solution}[1]{{{\color{blue}{\bf Solution:} {#1}}}}
\usepackage[usenames,dvipsnames,svgnames,table,hyperref]{xcolor}

\renewcommand*{\theenumi}{\alph{enumi}}
\renewcommand*\labelenumi{(\theenumi)}
\renewcommand*{\theenumii}{\roman{enumii}}
\renewcommand*\labelenumii{\theenumii.}


\begin{document}
\ctitle{2}{part 1}{feb 9,2017}
\author{Rohan Varma}
\date{2/9/17}
\maketitle
\vspace{-0.75in}

\section{Problem 1}
\begin{enumerate}
\item Problem 1a

\solution{Our training data are \[ X_1 = [-1,-1], y_1 = -1 \] \[ X_2 = [-1, 1], y_2 = -1\], \[X_3 = [1,-1], y_1 = -1\], \[ X_4 = [1,1], y_4 = 1\]
If we let our vector of parameters \[ \Theta = [w_0, w_1, b] \] and append a 1 as the last element of each training sample X, we obtain X as a 3 x 1 column vector and $\Theta$ as a 3 x 1 column vector. Then, we can obtain our predictions with \[ \Theta^{T}X_i = y_i, i = [1,2,3,4] \]. In particular, where s represents the sign function: \[ s(-w_0 -w_1 + b) = -1\], \[s(-w_0 + w_1 + b) = -1 \] \[s(w_0 -w_1 + b) =-1 \] \[s(w_0 + w_1 + b) = 1 \]
We can let $w_0 = 3, w_1 = 4,  b = -2$ which satisfies all of the equations. Another set of parameters that satisfies the equations are $w_0 = 7, w_1 = 9, b = -11. $
}

\vspace{1cm}
\item Problem 1b

\solution{No perceptron exists to compute the XOR function. This is because the data given by XOR are not linearly separable. Perceptrons can only separated data linearly, and the decision boundary is given by the plane $\Theta^TX=0$. However, the data for XOR looks as follows: \includegraphics{xor.png} \newline{} which we cannot draw a single line through to separate.}
\end{enumerate}

\newpage
\section{Problem 2}
\item Problem 2a

\solution{
We have the negative log-likelihood $J(\theta) = =\sum_{n=1}^{N} y_nlog(\sigma(\theta^{T}x_n)) + (1-y_n)log(1-\sigma(\theta^{T}x_n))$. Differentiating with respect to a parameter $\theta_j$, 
\[ \frac{\delta J}{\delta \theta_j} = -\sum_{n=1}^{N} y_{n}x_{n,j}(1 - \sigma(\theta^{T}x_n)) - x_{n,j}(1-y_n)(\sigma(\theta^{T}x_n)) \]
Simplifying, 
\[-\sum_{n=1}^{N}y_{n}x_{n,j} - y_{n}x_{n,j}h_{\theta}(x_n) - x_{n,j}h_{\theta}(x_n) + x_{n,j}y_{n}h_{\theta}(x_n) \]
\[ = -\sum_{n=1}^{N}x_{n,j}(y_{n} - h_{\theta}(x_n) \]
giving us
\[ \frac{\delta J}{\delta \theta_j}= \sum_{n=1}^{N} x_{n,j}(h_{\theta}(x_n) - y_n) \]
where $x_{n,j}$ denotes the jth feature in the nth training example. 
}
\vspace{1cm}
\item Problem 2b

\solution{
Substituting the sigmoid function and differentiating the quantity from 2A with respect to $\theta_j$, we have
\[
\frac{\delta^{2}J}{\delta \theta_j \theta_k} = \sum_{n=1}^{N} x_{n,j}x_{n,k}(\sigma(\theta^{T}x_n))(1-\sigma(\theta^{T}x_n))
\]
which is one of the components of the vector of partial second derivatives. This is some arbitrary element in the Hessian $H_{j,k}$. In general, the Hessian is composed of a vector of these $\theta_j, \theta_k$ partial derivatives, so we have $x_{n,j}, x_{n,k}$ as vectors, giving us their product as $x_{n}x_{n}^{T}$ for the n training example. This gives us the Hessian: 
\[ H = \sum_{n=1}^{N} h_{\theta}(x_n)(1 - h_{\theta}(x_n))x_{n}x_{n}^{T} \]
}
\vspace{1cm}

\item Problem 2c

\solution{
J is convex if $z^{T}Hz = \sum_{j,k}z_{j}z_{k}H_{j,k} \geq 0$
By substitution, we have:
\[\sum_{j,k}z_{j}z_{k} \sum_{n=1}^{N}x_{n,j}x_{n,k}\sigma(\theta^{T}x_n)(1-\sigma(\theta^{T}x_n)) \]
From 2b, we can rewrite the inner product, obtaining: 
\[\sum_{j,k}z_{j}z_{k}\sum_{n=1}^{N} h_{\theta}(x_n)(1 - h_{\theta}(x_n))x_{n}x_{n}^{T} \]. 
The inner product is greater than or equal to 0 since it is the product of the square of a vector's magnitude (always positive or 0) and two probabilities (which are both positive or 0). So we are left with \[\sum_{j,k}z_{j}z_{k}H, H \geq 0\]. Since we know $\sum_{j,k} z_{j}z_{k} = ||z||^{2} \geq 0$ by the definition of a vector's dot product with itself, and that sum in this case is just being multiplied each time by a number that's greater than or equal to 0, this quantity has to be greater than or equal to 0. 
}
\newpage

\section{Problem 3}
\item Problem 3a

\solution{
\[ \frac{\delta J}{\delta \theta_0} = 2\sum_{n=1}^{N} w_{n}(\theta_0 + \theta_{1}x_{n,1} - y_{n}) \]
\[ \frac{\delta J}{\delta \theta_1} = 2 \sum_{n=1}^{N} w_{n}x_{n,1}(\theta_0 + \theta_{1}x_{n,1} - y_{n}) \]
}

\vspace{1cm}
\item Problem 3b

\solution{
\[ \frac{\delta J}{\delta \theta_0} = 0 \]
\[ \theta_0\sum_{n=1}^{N}w_n + \theta_1\sum_{n=1}^{N}x_{n,1}w_{n} = \sum_{n=1}^{N}w_{n}y_{n} \]
We obtain an expression for $\theta_0$ in terms of $\theta_1$:
\[\theta_0 = \frac{\sum_{n=1}^{N}w_ny_n - \theta_1\sum_{n=1}^{N}x_{n,1}w_n}{\sum_{n=1}^{N}w_n} \]

Now solving explicitly for $\theta_1$
\[ \frac{\delta J}{\delta \theta_1} = 0 \]
\[\theta_0\sum_{n=1}^{N}w_nx_{n,1} + \theta_1\sum_{n=1}^{N}x_{n,1}^{2}w_n = \sum_{n=1}^{N}w_nx_{n,1}y_n \]
Substituting, 
\[ \frac{\sum_{n=1}^{N}w_ny_n - \theta_1\sum_{n=1}^{N}x_{n,1}w_{n}}{\sum_{n=1}^{N}w_{n}}\sum_{n=1}^{N}w_{n}x_{n,1} +  \theta_1\sum_{n=1}^{N}w_nx_{n,1}^{2} = \sum_{n=1}^{N}w_{n}x_{n,1}y_n \]
\[(\sum_{n=1}^{N}w_ny_n - \theta_1\sum_{n=1}^{N}x_{n,1}w_{n})(\sum_{n=1}^{N}w_{n}x_{n,1}) + \theta_{1} (\sum_{n=1}^{N}w_n)(\sum_{n=1}^{N}w_nx_{n,1}^{2}) = (\sum_{n=1}^{N}w_n)(\sum_{n=1}^{N}w_nx_{n,1}y_n) \]
Expanding and factoring to isolate $\theta_1$, we get:
\[\theta_1( (\sum_{n=1}^{N}w_n)(\sum_{n=1}^{N} w_nx_{n,1}^{2}) -  (\sum_{n=1}^{N}x_{n,1}w_n)^2     ) = (\sum_{n=1}^{N}w_n)(\sum_{n=1}^{N}w_nx_{n,1}y_n) - (\sum_{n=1}^{N}w_ny_n)(\sum_{n=1}^{N}w_nx_{n,1}) \]
Dividing to solve for theta 1, we get
\[\theta_1 = \frac{(\sum_{n=1}^{N}w_n)(\sum_{n=1}^{N}w_nx_{n,1}y_n) - (\sum_{n=1}^{N}w_ny_n)(\sum_{n=1}^{N}w_nx_{n,1})}{( (\sum_{n=1}^{N}w_n)(\sum_{n=1}^{N} w_nx_{n,1}^{2}) -  (\sum_{n=1}^{N}x_{n,1}w_n)^2     )} \]
Now we can go back to our expression for $\theta_0$ that was in terms of $\theta_1$, and plug the explicit form of $\theta_1$ back in to get $\theta_0$ explicitly as well.
}

\vspace{1cm}

\end{document}
